# SmartRAG_AI_ASST_Extern
# ðŸ“„ Smart RAG: A Document-Aware AI Assistant

**Externship Project â€“ Outamation | Extern Platform**  
_By Chelluri Sai Vishal (June 2025)_

A full Retrieval-Augmented Generation (RAG) system that classifies documents before answering user queries. The pipeline uses open-source tools like Mistral 7B, FAISS, MiniLM, and LangChain â€” all running locally in Colab without external APIs.

---

## ðŸš€ Overview

Smart RAG intelligently classifies documents like **Payslips**, **Bank Statements**, and **Appraisals** before routing user queries to the correct index. This ensures the system retrieves the most relevant content, improving both **speed** and **accuracy** of responses.

---

## ðŸ§  Architecture

User Query â†’ LLM Classifier â†’ Document Type â†’ Relevant Index â†’ RAG Engine â†’ Answer

![RAG Pipeline Architecture](rag_pipeline_architecture.png)

---

## ðŸ”§ Features

- âœ… Document-type classification with **MiniLM embeddings**
- âœ… Retrieval routing to **per-type FAISS vector stores**
- âœ… Answer generation using **Mistral 7B** (local GGUF model)
- âœ… Optimized chunking: 1000 characters with 200 overlap
- âœ… Runs locally in Google Colab (GPU) using `llama-cpp-python`

---

## ðŸ“‚ Project Structure
â”œâ”€â”€ Final_RAG_Pipeline_Vishal.ipynb # Main code notebook
â”œâ”€â”€ sample_contract.pdf # Example input PDFs
â”œâ”€â”€ bank_statement.pdf
â”œâ”€â”€ payslip.pdf
â”œâ”€â”€ rag_pipeline_architecture.png # Architecture image
â”œâ”€â”€ README.md # Project summary

---

## ðŸ§ª Sample Query Evaluation

| Query                             | Answered Correctly | Relevance | Time  |
|----------------------------------|---------------------|-----------|-------|
| What is the net salary?          | âœ… Yes              | â­â­â­â­â­     | 3.5s  |
| How much was the last transaction? | âœ… Yes            | â­â­â­â­      | 1.0s  |
| What is the estimated home value? | âŒ No              | âŒ        | â€”     |

ðŸ’¡ *Insight:* Misclassification leads to total failure â†’ routing is crucial.

---

## ðŸ“¦ Tech Stack

- **Mistral 7B (GGUF)** â€“ via `llama-cpp-python`
- **FAISS** â€“ vector search for chunked document retrieval
- **MiniLM (SentenceTransformers)** â€“ fast and semantic embeddings
- **LangChain** â€“ integrates LLM with retrieval and prompt flow
- **PyMuPDF** â€“ for PDF text extraction
- **Google Colab** â€“ runtime for GPU testing

---

## ðŸ“ˆ Lessons Learned

- Classification is *more important* than model size.
- Chunking and retrieval outweigh pure LLM power.
- Open-source LLMs like Mistral are production-ready with the right setup.

---

## ðŸ”® Future Work

- ðŸ”„ Add **hybrid retrieval** (keyword + vector)
- ðŸ” Integrate **reranking layer** using LLMs
- ðŸŒ Deploy using **Gradio** for live demo
- ðŸ§± Switch to **Qdrant or Weaviate** for persistent vector storage

---

> ðŸ“Œ _Project submitted for review under Outamationâ€™s Externship via Extern. Repository is private until completion._

